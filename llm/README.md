# LLM

Here you can find quantized [`openbmb/MiniCPM-V-4_5-gguf`](https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf) weights.
You can use any model you want, but this one was chosen cause of:
- image support
- fast inference in comparison with other multimodal LLM

Also, you can check some useful links:
- [LLaMa.cpp OpenAI-like server inference](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md)
- [Official MiniCPM cookbook for a llama.cpp inference](https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-v4_5_llamacpp.md)
