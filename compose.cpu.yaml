services:

  llm:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-server
    ports:
      - "8000:8000"
    volumes:
      - ./llm:/models
    environment:
      LLAMA_ARG_MODEL: /models/ggml-model-Q4_K_M.gguf
      LLAMA_ARG_MMPROJ: /models/mmproj-model-f16.gguf
      LLAMA_ARG_CTX_SIZE: 1024
      LLAMA_ARG_KV_UNIFIED: 1
      LLAMA_ARG_SPECULATIVE: 0
      LLAMA_ARG_N_PARALLEL: 8
      LLAMA_ARG_MLOCK: 0 
      LLAMA_ARG_N_THREADS: 8 
      LLAMA_ARG_NO_WARMUP: 1
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_PORT: 8000

  redis:
    image: redis:7-alpine
    container_name: redis-cache
    ports:
      - "6379:6379"
    command: >
      redis-server
      --save ""
      --appendonly no
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru

  backend:
    build:
      context: ./backend
    platform: linux/amd64
    container_name: go-backend
    depends_on:
      - llm
    ports:
      - "8080:8080"
    environment:
      CACHE_ENABLE: "true"

      SERVER_PORT: "8080"
      SERVER_TIMEOUT: "5m"
      SERVER_THROTTLE_LIMIT: "5"

      OPENAI_API_KEY: "local-no-key"
      OPENAI_BASE_URL: "http://llm:8000/v1"
      OPENAI_MODEL: "local-model"

      REDIS_ADDR: "redis:6379"
      REDIS_PASSWORD: ""
      REDIS_DB: "0"
      REDIS_TTL: "10m"

  frontend:
    build:
      context: ./frontend
    container_name: gradio-front
    restart: unless-stopped
    depends_on:
      - backend
    ports:
      - "8090:8090"
    environment:
      BACKEND_URL: "http://backend:8080"
      GRADIO_HOST: "0.0.0.0"
      GRADIO_PORT: "8090"
      GRADIO_MAX_FILE_SIZE: "1mb"
